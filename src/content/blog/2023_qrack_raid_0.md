---
title: "How to build a ~40 qubit simulator for less than $3000 (and then simulate 54 qubits on it)!"
author: Dan Strano
day: 25
month: 12
year: 2023
---

At 32-bit floating point precision (with 64-bit complex numbers), 32 *giga*bytes of memory will fit 32 qubits worth of state vector simulation. For each additional qubit desired, you need to _double_ the memory (compounded per additional qubit). 40 qubits, depending on precision, therefore requires 8 or 16 *tera*bytes (_256 or 512 times_ as much RAM). Unfortunately, typical ram DIMMs might come in 16 gigabyte increments (give-or-take up to a couple of factors of 2), and maybe you’ll fit 8 of them in a server motherboard, for 128 gigabytes. Alternatively, NVIDIA A100 GPUs might carry 80 gigabytes apiece, you might fit about 64 gigabytes worth of quantum state vector simulation in a single GPU, and then it will take 128 of these (with high-speed interconnects) to reach 8 terabytes, at a cost of thousands or tens of thousands of dollars per hour to rent. What is the hobbyist quantum computing researcher to do?

Believe it or not, for about $2,500, you can build your “dream” rig for gate-based quantum computer simulation up to 40 qubits and beyond (albeit significantly slower than a GPU cluster, but close to the processing throughput rate of a large CPU)! In this blog post, we’ll describe the construction of such a device and introduce you to how Unitary Fund’s Qrack simulator can use “novel” and approximate simulation techniques to push about 16 TB of high-speed swap disk from 40 qubits of state vector simulation into 54-qubit territory, potentially rivaling the 2019 Sycamore “quantum supremacy” experiments for fidelity and cost!

IBM was likely the first to point out, after the 2019 Sycamore universal random circuit sampling experiments, that disk storage could be employed to increase effective RAM limits of quantum computer simulation far past what is attainable with GPU and “DIMMs” (RAM boards inserted into your computer motherboard). Roughly, the largest supercomputers in the world, even arrayed together, might only have DRAM capacity for about 49 or 50 qubits of state vector simulation, aside from other techniques like tensor networks or any approximate simulation techniques. (Just as with your personal computer hardware, adding 1 qubit _doubles_ the necessary memory footprint, and the capacity of the largest supercomputers in the world, today, does not double in negligible time, effort, or cost!) However, disk storage can be _nearly_ as good as “DIMMs,” but drastically cheaper and far easier to scale to terabyte-range and past!

The obvious problem is that data transfer to and from disk is _far_ slower than “DIMMs.” Ultimately, “swap disk” as (random-access) memory needs to somehow be made fast enough to keep up with the processing throughput capacity of whatever CPU(s) to which it’s attached. However, Unitary Fund and the Qrack simulator development team have demonstrated proof-of-concept for creating swap disk resources that are _nearly_ fast enough to keep up with a 64-hyper-thread AMD Epyc processor: this prototype machine uses “RAID 0” configuration on 8 (“consumer-grade”) NVMe solid-state drives, achieving 8 times the throughput of a single drive! By “striping” the (single) logical swap disk partition across 8 hardware drives, interspersing small segments of memory in a pattern of “stripes” that are likely to be accessed at once across physical drives, the overall transfer speed nets to the _sum_ of the transfer speeds of each respective disk, achieving speeds close to sufficient to keep the CPU barely fully utilized on large state vector simulations (particularly with OS-level swap disk compression enabled, such as by manually enabling “zswap” with “zstd” compressor in Linux)!

Without suggesting specific brands by name, the concept for the bill of materials is simple:
- You will need about 8 NVMes (of the same brand and level of wear-and-tear) at perhaps 2 TB apiece (for about $1,200).
- These 8 NVMes can sit in two 4-way PCIe motherboard slot splitters (at about $40 for 2 splitters, and hardware RAID is not necessary) on 16-lane PCIe slots, for 4 PCIe lanes per NVMe.
- The NVMes are logically combined into a single RAM volume with “RAID 0” “disk striping” (by software configuration, such as available in Ubuntu operating system, for free). 
- A CPU with many hyper-threads (maybe 64) should be selected to be just fast enough to keep up with the net throughput rate of 32 PCIe lanes. (We bought a 64-hyper-thread AMD Epyc CPU, ~112 GB of DIMMs, and a motherboard with 2 slots at 16 PCIe lanes apiece and 4-way splitting support in BIOS as a bundle for about $800.)
- We strongly recommend a chassis with built-in cooling fans, as for a gaming desktop (at about $100, depending on your motherboard form factor, and, by the way, it could come with an RGB accessory).
- You need a CPU cooler, but don’t go overboard, and make sure it fits in your chassis (for <$65).
- Similarly, we strongly recommend a (small, “low-TDP”) dedicated cooling solution for the NVMe array, but we’ve found that a vertically slot-mounted double mechanical fan over the splitters is perfectly sufficient in a chassis with built-in fans (at ~$20 for the slot-mounted fan).
- You will also need a power supply with sufficient wattage to reliably support all components. (850W might meet your needs, for about $150.)

NVMes (based on NAND flash) are relatively cheap to manufacture and purchase by now. Your motherboard needs to support 4-way splitting of PCIe slots, if you use 4-way splitters, and your motherboard slots should probably have at least 16 PCIe lanes apiece. As we said, you might want a chassis with built-in mechanical fan cooling, commonly marketed as for those building gaming PCs, and you’ll likely also want a CPU cooler and some form of cooling specifically for the NVMe splitters, for which we used a vertical slot-mounted fan at a right angle to our two splitter boards. (Your chassis must provide such a vertical slot over the NVMe splitter PCIe slots, if you intend to follow our example.) No GPU is necessary! That’s about $2,000 for the key components of NVMes, motherboard, CPU, and DIMMs; budget up to $3,000 for the build and expect to walk away with about $500 left over, but $2,500 is about what we paid for our prototype, and a slightly better version of it could still be built for about $2,500, a year later.

When using the Qrack gate-based quantum computer simulator library, we officially suggest 64-bit “double” precision (with 128-bit complex numbers) when state vector simulation will exceed about 32 qubits: 16 TB of swap disk will realistically support a single state vector simulation up to 8 TB in width, giving you 39 qubits. However, we want to do something even more ambitious: we want our $2,500 machine to compete with 2019 Sycamore! In this case, we don’t need near-perfect fidelity of simulation, and we can approximate a circuit comparable to that Sycamore experiment with 32-bit “float” precision (with 64-bit complex numbers) and recourse to Qrack’s novel “hybrid” algorithms for simulation, including “quantum binary decision diagrams” (QBDD), based on the work of Robert Wille and team at Jülich Supercomputing Centre!

You can experiment with your own Qrack benchmarks, but, if you’re looking for a ready-made case of universal random circuit sampling similar to 2019 Sycamore, check out the case “test_noisy_fidelity_2qb_nn_estimate” in Qrack’s C++ benchmark suite: it runs Qrack’s “Schmidt-decomposition rounding parameter” (SDRP) approximation technique at successively lower levels of “rounding,” and therefore successively higher overall fidelity, until failure due to “out-of-memory.” While the gate set used is not exactly that of the 2019 Sycamore experiment, this set is designed to (likely) be more expressive in less circuit depth while being easier to simulate, on average. It intersperses single-qubit gate layers with maximally densely packed 2-qubit “nearest-neighbor” couplers across all qubits on a (close to) “square chip” similar to the 2019 Sycamore experiment topology. Each single-qubit gate entails a random Pauli basis transformation across all Pauli bases, composed with a uniformly random variational “RZ” gate (Z-axis rotation), such that it could take as few as 3 layers of single-qubit gates to achieve a completely general 3-parameter unitary (“U”) gate. Each 2-qubit gate is a coupler on a nearest neighbor topology, with a quarter of all gate options being the 3 controlled Pauli gates, a quarter being 3 “anti-controlled” Pauli gates (being gated by “0” control state instead of “1” control state), and half of all gates being variants of the “swap” gate with 6 physically distinct choices of phase effects in addition to the “swap” component, with a 50/50 “coin-flip” chance of choosing either direction between control and target qubits for all 2-qubit gates.

This circuit and benchmark was explored on a single GPU at 54 qubits “width” and various layers of “depth” up to about 10 layers in our report “[Exact and approximate simulation of large quantum circuits on a single GPU](https://arxiv.org/abs/2304.14969).” (2019 Sycamore ultimately explored up to 20 layers.) On a single GPU with 80 GB, Qrack managed to achieve about 4% _average_ fidelity on 100 trials at 7 depth layers, while _median_ fidelity was actually much closer to 0%, suggesting that some smaller fraction of instances of this random-circuit generation protocol are much easier to simulate (at much higher than 4% fidelity) than the majority of cases. Qrack is actually able to know simulation fidelity from “first principles” on the theoretical basis of how “SDRP” rounding events occur and the fact that each single rounding event has a very mathematically simple effect on overall circuit fidelity, reaffirmed by experimental check at much lower circuit widths that are possible to simulate in the ideal, without “noise,” within typical system memory limits. Each SDRP series single trial could take days or even a couple of weeks to run: we’ve completed about 2 trials, with 1 more underway (with admitted potential for significant exploratory selection bias and random statistical variation at <3 samples collected). With the combination of “QTensorNetwork,” “QUnit,” “QStabilizerHybrid,” and “QBdtHybrid,” simulation algorithm “layers,” operating together in concert, 2 trials resulted in 100% fidelity, and the ongoing trial is currently at 6% fidelity, not yet at exhaustion of memory for potentially days to come. More data collection is underway (slowly), but, with this low sample size vs. 100 trials on a single GPU, these results might be more representative of _median_ fidelity than average, such that we might guess that _most_ trials achieve closer to 6% fidelity than 0%, while many easier trials of lower frequency should drive the _average_ fidelity even higher than this!

There’s no “free lunch” or “silver bullet” for this “BQP-complete” problem of universal random circuit sampling: if you undertake this hardware build and experiment, expect to spend months collecting simulated results to reach a sufficient sample count for a study, whereas the 2019 Sycamore study took about a couple of _minutes._ However, it’s worth noting: once the unitary “preamble” of the circuit is fully simulated, before terminal measurement, there is a comparatively tiny simulation cost per measurement shot, such that simulating the exact same circuit for one measurement shot vs. a million measurement shots costs almost the same overhead and time! Then, despite having to wait days or weeks per circuit, the overall simulation time as a multiple of hardware execution time is actually not all that severe within and across the domain of computer science, and you very well might lead _cost_ projections to execute universal circuits like these at some of the highest fidelity values ever attained _at all,_ in any experiment performed to date!

Come visit us in the “#qrack” channel on the [Unitary Fund Discord server](https://discord.com/invite/JqVGmpkP96) for tips and discussion, for anything related to the Qrack simulator or the hardware build discussed in this blog post! We’re a friendly bunch, and we’re very excited to hear more about who’s using Qrack, and about the experiments you’re designing, and the use cases you’re finding!

Happy Holidays from Unitary Fund, and you’ll hear more about our experiments with RAID 0 swap disk arrays for quantum computer simulation in 2024! [Come join the discussion!](https://discord.com/invite/JqVGmpkP96)
